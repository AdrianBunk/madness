\section{Load and memory balancing}
Load and memory balancing is a critical issue on current generation of shared and distributed memory computers. \ On
many terascale and petascale computer there is no virtual memory capabilities on the compute nodes so memory management
is very important.

Poor distribution of work (load imbalance) is the largest reason for inefficient parallel execution within MADNESS.
\ Poor data distribution (data imbalance) contributes to load imbalance and also leads to out-of-memory problems due to
one or more processes having too much data. \ Thus, we are interested in uniform distribution of both work and data. 

Many operations in MADNESS are entirely data driven (i.e., computation occurs in the processor that
"owns" the data) since there is insufficient work to justify moving data between processes
\ (e.g., computing the inner product between functions). \ However, a few expensive operations can have work shipped to
other processors. 

There are presently three load balancing mechanisms within MADNESS 

\begin{itemize}
\item static and driven by the distribution of data, 
\item dynamic via random assignment of work, and 
\item dynamic via work stealing (currently only in prototype). 
\end{itemize}
Until the work stealing becomes production quality we must exploit the first two forms. \ The random work assignment is
controlled by options in the FunctionDefaults class. 

\begin{itemize}
\item \texttt{FunctionDefaults::set\_apply\_randomize(bool) }controls the use of randomization in applying integral
(convolution) operators. It is typically beneficial when computing to medium/high precision. 
\item \texttt{FunctionDefaults::set\_project\_randomize(bool)} controls the use of randomization in projecting from an
analytic form (i.e., C++) into the discontinuous spectral element basis. \ It is typically \ beneficial unless there is
already a good static data distribution. \ Since these options are straightforward to enable, this example focuses on
static data redistribution. 
\end{itemize}
The process map (an instance of \texttt{WorldDCPmapInterface}) controls mapping of data to processors and it is actually
quite easy to write your own (e.g., see \texttt{WorldDCDefaultPmap }\texttt{or LevelPmap}) that ensure uniform data
distribution. \ However, you also seek to incorporate estimates of the computational cost into the distribution. \ The
class \texttt{LBDeuxPmap} (deux since it is the second such class) in \texttt{trunk/src/lib/mra/lbdeux.h} does this by
examining the functions you request and using provided weights to estimate the computational cost. 

Communication costs are proportional to the number of broken links in the tree. \ Since some operations work in the
scaling function basis, some in the multiwavelet basis, and some in non-standard form, there is an element of
empiricism in getting best performance from most algorithms. 

The example code in \texttt{trunk/src/apps/examples/dataloadbal.cc} illustrates how the discussions in this section can
be applied.

\section{How to ``think'' MADNESS}
MADNESS is based on multiresolution analysis (MRA) and low-separation rank (LSR) approximations of functions and
operators. \ It can be considered an adaptive spectral elemen method using a discontinuous and singular multiresolution
basis. \ The representations of differential operators in the wavelet bases are provably similar to adaptive finite
difference discretizations. \ Thus, the process of solving the resulting linear systems will have similar behaviors as
in other adaptive methods. \ For example, the derivative operator and the Laplacian operator are unbounded operators.
\ Thus the condition number, which often constraints how accurately the linear system can be solved, goes to infinity
as the bases or the nets are refined. \ In order to solve these equations in practice, one has to precondition the
system. \ Effective preconditioners are problem dependent and the theory of their construction is an area of on-going
research.

The integral operator, which is the formal inverse associated to the differential operator, are usually bounded
operator. \ MRA and LSR have been proven to be one of the techniques that can effectively apply some of the physically
important operators and their kernel fast and with ease.

Two of the most important operators that we illustrate in this manual are the Poisson operator (in section 5), and the
\ Helmholtz operator. \ The heat equation is ...

\subsection{Solve the integral equation}
In many situations the integral operator associated to the differential operator have an analytic kernel. \ The simplest
examples are the convolution operators. \ In equation (9) the free-space Poisson equation is converted to a convolution
with the Poisson (or Coulomb) kernel, the Schrodinger equation with potential  $V$ is converted to a Lippman-Schwinger
equation using the bound-state Helmholtz kernel, and the last example applies Duhamel's principle to write a time
dependent differential equation with linear operator  $\hat{L}$ and a non-linear operator  $N$ as a semi-group
equation.


\begin{align*}\label{eqn:refText8}
\nabla ^{2}u=-4\pi \rho                     & \to u=G\ast \rho    \ \ \    \textrm{where} G(r-r')=\frac{1}{|r-r'|} \\
-{\frac{1}{2}}\nabla ^{2}\psi +V\psi =E\psi & \to \psi =-2G\ast V\psi \ \ \ \textrm{where} G(r-r')=\frac{e^{-\sqrt{-2E|r-r'|}}}{4\pi |r-r'|}\\
\hat{L}u+N(u,t)=\dot{u}                     & \to u(t)=e^{\hat{L}t}u(0) +\int _{0}^{t}e^{\hat{L}(t-t')}N(u,t')\mathit{dt}' &
\end{align*}

Most codes, including MADNESS, are bad at solving differential equations to high accuracy -- this is why there is so
much emphasis placed on finding a good pre-conditioner. The problem arises from the spectrum of the differential
operator. Consider the Laplacian in 1D acting on a plane wave 

\begin{equation}
\frac{d^{2}}{\mathit{dx}^{2}}e^{i\omega x}=-\omega ^{2}e^{i\omega x}
\end{equation}
The effect of the Laplacian is to greatly amplify high frequencies  $\omega $ (where most of the numerical error lies)
whereas physical applications are primarily interested in lower frequencies. The eigenvalues of the corresponding
inverse or integral operator have the opposite effect -- high frequencies are suppressed and lower frequencies
emphasized.

The integral form is potentially better in many ways -- accuracy, speed, robustness, asymptotic behavior, etc.. If you
really, really, want to solve the differential form, then instead of using the phrase ``integral form'' say ``perfectly
preconditioned differential form'' so that you can do the right thing.

\subsection{Carefully analyze discontinuities, noise, singularities, and asymptotic forms}
Your function needs to be evaluated at close to machine precision. \ The higher the order of the basis ( $k$) the
greater the necessary accuracy regardless of what threshold you are trying to compute to. \ The accuracy and
convergence of the Gauss-Legendre quadrature rests on the function being smooth (well approximated by a polynomial) at
some level of refinement. Discontinuities in the function value or derivatives, \ singularities, and numerical noise
can all cause excessive refinement as MADNESS tries to deliver the requested precision-Gibbs effect in action.
\textit{The usual symptoms of this problem are unexpectedly slow execution and excessive memory use.} Here are some
tips to work with these effects.

Discontinuities and singularities need to be consciously managed. Integrable point singularities might sometimes work
unmodified (e.g.,  $1/r$ in 3-D) but can unpredictably fail, e.g., if a quadrature point lands very near the
singularity by accident. If possible, arrange for such points/surfaces to coincide with dyadic points (i.e., an integer
multiple of some power of two division of the domain) -- this will give the most accurate representation and exploits
the discontinuous spectral basis. If you cannot ensure such placement, you must manually or analytically regularize the
function and one would usually employ a parameter to control the length scale of any problem modification and to enable
systematic demonstration of convergence. E.g., eliminate the cusp in an exponential with

\begin{equation}
\exp (-r)\to \exp (-\sqrt{r^{2}+\sigma ^{2}})
\end{equation}
or replace a step function with 

\begin{equation}
\theta (x)\to \theta (x,\lambda )=\frac{1}{2}(1+\tanh \frac{x}{\lambda })
\end{equation}
or the Coulomb potential in 3-D with 

\begin{equation}
\frac{1}{r}\to u(r,c)=\frac{1}{r}\operatorname{erf}\frac{r}{c}+\frac{1}{c\sqrt{\pi
}}e^{-\left(\frac{r}{c}\right)^{2}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \int _{0}^{\infty
}\left(u(r,c)-r^{-1}\right)r^{2}\mathit{dr}=0
\end{equation}
(the integral indicates that the mean error is zero independent of  $c$).

Numerical noise can be a problem if your function is evaluated using interpolation or some other approximation scheme,
or when switching between representations (e.g., between forms suitable for small or large arguments). If you are
observing inefficient projection into the basis, ensure that your approximation is everywhere smooth to circa 1 part in
 $10^{12}$ or better.

MADNESS itself computes to a finite precision and when computing a point-wise function of a function (i.e.,  $g(f(x))$
where  $f(x)$ is a MADNESS function and  $g(s)$ is a user-provided function) the user-provided function must tolerate
that approximation within tolerance or noise. A classic example is computing the function

\begin{equation}
V(\rho (x))=\frac{C}{\rho ^{1/3}(x)}
\end{equation}
where in the original problem one knows that  $\rho (x)>0,\ \forall x$ but numerically this positivity not guaranteed.
In this case an effective smoothing is 

\begin{equation}
\begin{gathered}V(\rho )\to V(S(\rho ))\\S(s)=\left\{\begin{matrix}s_{0}&s\le 0\\q(s,s_{0,}s_{1})&0<s\le
s_{1}\\s&s>s_{1}\end{matrix}\right.\\q(s,s_{0},s_{1})=s_{0}-(-2s_{1}+3s_{0})\left(\frac{s}{s_{1}}\right)^{2}+(2s_{0}-s_{1})\left(\frac{s}{s_{1}}\right)^{3}\end{gathered}
\end{equation}
The function  $S(s)$ coincides with its argument for  $s>s_{1}$ and for smaller values smoothly switches to a minimum
value of  $s_{0}$ with continuous value and derivative at both end points.

Some computations are intrinsically expensive. For instance, the function  $\exp (i\omega r)$ is oscillatory everywhere
and the number of required coefficients will increase linearly with the solution volume. \ In a 3-D box of width  $L$,
the number of coefficients will be  $O\left(\left(Lk\omega \right)^{3}\right)$ (where  $k$ is the multiwaveler or
polynomial order). For  $L=1000$,  $k=12$ and  $\omega =3$, a few hundred TB of data (i.e., too big!) will be
generated. Thus, it is worth making a back of the envelope estimate about the expected cost of computation before
getting started.

\subsection[Robustly trade precision for speed]{Robustly trade precision for speed}
TO BE COMPLETED.

\subsection{Choice of polynomial order (k)}
TO BE COMPLETED.

\section{Environment variables}
\texttt{MAD\_BIND} -- Specifies the binding of threads to physical processors. On both the Cray-XT and the IBM BG/P the
default value should be used. On other machines there is sometimes a small performance gain to be had from forcing
threads to use the same processor, thereby improving cache locality. The value is a character string containing three
integers in the range. The first indicates the core to which the main thread should be bound, the second the core for
the communication thread, and the third the core for first thread in the pool. Subsequent threads use successively
higher cores. A value of -1 indicates do not bind. The default on the XT is ``\texttt{1 0 2}{}'' and on the BG/P
``\texttt{{}-1 -1 -1}{}''.

\texttt{MAD\_NUM\_THREADS} -- Specifies the total number of threads to be used by each MPI process. If running with just
one MPI processes, there will be this many threads executing the application code so the minimum value is one. If
running with more than one MPI processes, one thread is dedicated to communication so the minimum value is two. The
default value is the number of processors detected (using this default is the only way presently to have different
numbers of threads on different nodes).

\texttt{MRA\_DATA\_DIR} -- Specifies the directory that contains the MADNESS data files (notably the autocorrelation
coefficients, two-scale coefficients, and Gauss-Legendre points and weights). Sometimes the compiled-in default must be
overridden. Only MPI process zero will use this.

\texttt{POOL\_NTHREAD} (deprecated) -- Specifies the number of threads in the compute pool. New users should set
\texttt{MAD\_NUM\_THREADS} instead.
